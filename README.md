# Bengali RAG System ü§ñ

A Simple Multilingual Retrieval-Augmented Generation (RAG) System for English and Bengali queries.

## Project Description

This project implements a basic RAG pipeline that can understand and respond to both English and Bengali queries. The system retrieves relevant information from a PDF document corpus (HSC26 Bangla 1st paper) and generates meaningful answers grounded in the retrieved content using advanced embedding techniques and GROQ LLM.

## Dataset

**Source**: HSC26 Bangla 1st Paper PDF  
**Link**: `data/hsc26_bangla1.pdf` 
**Pages Used**: 6-27 (Main ‡¶ó‡¶¶‡ßç‡¶Ø content only)

### Dataset Analysis

The dataset contains the main ‡¶ó‡¶¶‡ßç‡¶Ø ‡¶™‡¶æ‡¶∞‡ßç‡¶ü from HSC26 Bangla 1st paper. I have excluded other pages through code to focus only on the relevant prose content for better accuracy and relevance in the RAG system.

## Preprocessing
- **Text Extraction**: Used `pdfplumber` for accurate Bengali text extraction from pages 6‚Äì27
- **OCR Fallback**: Used `pytesseract` with Bengali language (ben) for OCR when text extraction failed
- **Text Cleaning**: Removed extra spaces, multiple newlines, tab characters, and formatting issues using regex
- **Encoding**: Maintained UTF-8 encoding for proper Bengali character support
- **Filtering**: Removed lines shorter than 15 characters to eliminate OCR noise and non-semantic content
- **Normalization**: Standardized Bengali punctuation (e.g., ‡•§), unified quotes, dashes, and brackets
- **Page Range Limiting**: Focused only on pages 6‚Äì27 (‡¶ó‡¶¶‡ßç‡¶Ø section), excluding irrelevant content like poetry or grammar
- **Stopword Removal**: Applied Bengali and English stopword filtering to improve semantic qu-ality during chunking
- **Whitespace Stripping**: Trimmed leading/trailing whitespaces from every line
- **Sentence Merging**: Merged fragmented lines from OCR into full sentences for better context retention
- **Pre-check for Noise**: Skipped lines with only numbers, symbols, or formatting artifacts
- **Consistent Line Breaks**: Ensured uniform paragraph spacing and newline structure for clean chunking later

## Chunks

**Strategy**: Paragraph-based chunking 
**Rationale**: This approach maintains semantic coherence while keeping chunks manageable for embedding and retrieval.

## RAG Implementation

- **Embedding Model**: `paraphrase-multilingual-MiniLM-L12-v2` for Bengali-English support
- **Vector Store**: FAISS for efficient similarity search
- **LLM**: GROQ `Llama-3.3-70b-versatile` for answer generation
- **Stop Words**: Removed common Bengali and English stop words for better query processing
- **Similarity**: Cosine similarity for semantic matching

## üöÄ Main Application (main.py)

- **Flask REST API** with CORS support
- **Real-time Web Interface** with modern UI
- **Multi-metric Evaluation**: Confidence, keyword matching, semantic similarity
- **Status Monitoring**: System health and performance tracking

## üõ†Ô∏è Project Setup

### Project Structure
```
rag-bangla-hsc/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ chunk_text.py                 # paragraph-based chunking
‚îÇ   ‚îú‚îÄ‚îÄ clean_text.py                 # clean text with ocr
‚îÇ   ‚îú‚îÄ‚îÄ embed_chunks.py               # vector database
‚îÇ   ‚îú‚îÄ‚îÄ extract_pdf_text.py           # extract pdf to text (page 6-27)
‚îÇ   ‚îú‚îÄ‚îÄ rag_model.py                  # rag model
‚îÇ   ‚îú‚îÄ‚îÄ search_qa.py                  # search from chunks for testing purpose 
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.pkl              # finalize vector.pkl
‚îÇ   ‚îú‚îÄ‚îÄ vector_store_small.pkl        # for testing purpose 
‚îÇ   ‚îî‚îÄ‚îÄ vector_store_2line.pkl        # for testing purpose 
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ hsc26_bangla1.pdf             # dataset
‚îÇ   ‚îú‚îÄ‚îÄ extracted_text.txt            # the text extract from pdf
‚îÇ   ‚îú‚îÄ‚îÄ cleaned_text.txt              # ocr text with pre-processing
‚îÇ   ‚îú‚îÄ‚îÄ chunks.txt                    # paragraph based chunking (final used)
‚îÇ   ‚îú‚îÄ‚îÄ chunks_small.txt              # line based chunking (for testing)
‚îÇ   ‚îî‚îÄ‚îÄ chunks_2line.txt              # 2 line based chunking (for testing)
‚îú‚îÄ‚îÄ .env                              # create .env file and paste your groq api key
‚îú‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ requirements.txt
```

### Installation & Setup

Full Setup Guide (TesserOCR + Bengali OCR)

1. Install Tesseract Engine (System Level)
- **Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install tesseract-ocr libtesseract-dev
sudo apt install tesseract-ocr-ben  # Bengali language data
```
- **Windows:**
- Download the Tesseract installer
- During installation, select Bengali language data.
- Add Tesseract path (e.g., C:\Program Files\Tesseract-OCR) to your system PATH.

- **macOS:**
```bash
brew install tesseract
brew install tesseract-lang  # or: brew install tesseract-lang && tesseract --list-langs
```
- For Bengali:
```bash
brew install tesseract-lang
# or, if not available:
wget https://github.com/tesseract-ocr/tessdata/raw/main/ben.traineddata -P /usr/local/share/tessdata/
```

1. **Clone the Repository**
```bash
git clone https://github.com/Arifuzzaman-Swapnil/Rag-Bangla-AI.git
cd Rag-Bangla-AI
```

2. **Create Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install Dependencies**
```bash
pip install -r requirements.txt
```

4. **Environment Setup**
```bash
# Create .env file and add your GROQ API key
echo "GROQ_API_KEY=your_groq_api_key_here" > .env
```

5. **Run the Processing Pipeline**
```bash
# Extract text from PDF
python app/extract_pdf_text.py  # it will generate data/extracted_text.txt

# Clean the extracted text
python app/clean_text.py        # it will generate data/cleaned_text.txt

# Create chunks
python app/chunk_text.py        # it will generate data/chunks.txt

# Generate embeddings and vector store
python app/embed_chunks.py     # it will generate app/vectore_store.pkl
```

6. **Start the Application**
```bash
python main.py
```

7. **Access the Application**
- Open your browser and go to: `http://localhost:8000`
- API endpoint: `http://localhost:8000/api/search`

## üì± Sample Outputs

### Query 1: ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?
### Expected Answer: ‡¶∂‡¶Æ‡ßç‡¶≠‡¶®‡¶æ‡¶• ‡¶¨‡¶æ‡¶¨‡ßÅ
![Query 1 Output](screenshots/supurush.png)

### Query 2: ‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?
### Expected Answer: ‡¶Æ‡¶æ‡¶Æ‡¶æ
![Query 2 Output](screenshots/3.png)

### Query 3: ‡¶π‡¶∞‡¶ø‡¶∂ ‡¶ï‡ßã‡¶•‡¶æ‡ßü ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?
### Expected Answer: ‡¶ï‡¶æ‡¶®‡¶™‡ßÅ‡¶∞‡ßá
![Query 3 Output](screenshots/horish_kaj.png)

### Query 4: ‡¶¨‡¶ø‡ßü‡ßá ‡¶â‡¶™‡¶≤‡¶ï‡ßç‡¶∑‡ßá ‡¶ï‡¶®‡ßç‡¶Ø‡¶™‡¶ï‡ßç‡¶∑‡¶ï‡ßá ‡¶ï‡ßã‡¶•‡¶æ‡ßü ‡¶Ü‡¶∏‡¶§‡ßá ‡¶π‡¶≤‡ßã?
### Expected Answer: ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ
![Query 3 Output](screenshots/kolikata.png)

### Query 5: "‡¶Æ‡ßá‡ßü‡ßá ‡¶Ø‡¶¶‡¶ø ‡¶¨‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ñ‡¶æ‡¶∏‡¶æ ‡¶Æ‡ßá‡ßü‡ßá ‡¶Ü‡¶õ‡ßá"- ‡¶â‡¶ï‡ßç‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡¶æ‡ßú?
### Expected Answer: ‡¶π‡¶∞‡¶ø‡¶∂
![Query 3 Output](screenshots/ukti.png)

### Query 6: ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶Ü‡¶∏‡¶≤ ‡¶Ö‡¶≠‡¶ø‡¶¨‡¶æ‡¶¨‡¶ï ‡¶ï‡ßá?
### Expected Answer: ‡¶Æ‡¶æ‡¶Æ‡¶æ
![Query 3 Output](screenshots/onupom_obhibakol.png)

### Query 7: ‡¶ï‡¶®‡ßç‡¶Ø‡¶æ‡¶ï‡ßá ‡¶Ü‡¶∂‡ßÄ‡¶∞‡ßç‡¶¨‡¶æ‡¶¶ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶æ‡¶ï‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡¶®‡ßã ‡¶π‡¶≤‡ßã?
### Expected Answer: ‡¶¨‡¶ø‡¶®‡ßÅ‡¶¶‡¶æ
![Query 3 Output](screenshots/binu.png)

## ‚ùì Technical Q&A

### Q1: What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?
## ‚ùì Technical Q&A

### Q1: What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?

**Answer**: I used `pdfplumber` library for text extraction because it provides excellent support for Bengali Unicode characters and maintains text formatting better than other libraries like PyPDF2. For OCR-related formatting issues, I integrated `tesseract-ocr` and `poppler-utils` for better Bengali text recognition.

#### **Libraries Used:**
```python
import pdfplumber  # Main text extraction
import re          # Text cleaning and formatting
# Additional OCR support: tesseract-ocr, poppler-utils
```

#### **Text Extraction Code:**
```python
import pdfplumber

# Extract specific pages (6-27) for main ‡¶ó‡¶¶‡ßç‡¶Ø content
with pdfplumber.open('data/hsc26_bangla1.pdf') as pdf:
    for i in range(5, 27):  # 0-indexed pages
        page = pdf.pages[i]
        text = page.extract_text()
        if text:
            all_text.append(text)
```

#### **Main Formatting Challenges & Solutions:**
The main formatting problems faced were OCR-related issues, which were solved by `tesseract-ocr` and `poppler-utils`:
- Multiple newlines and extra spaces
- Inconsistent line breaks  
- Page headers/footers mixing with content
- Special Bengali characters encoding
- OCR misrecognition of Bengali fonts

```python
import re

# 1. Multiple newlines ‚Üí Single newline
text = re.sub(r'\n+', '\n', text)

# 2. Multiple spaces ‚Üí Single space  
text = re.sub(r'[ \t]+', ' ', text)

# 3. Remove trailing/leading spaces from lines
text = re.sub(r' +\n', '\n', text)
text = re.sub(r'\n +', '\n', text)
```

**Key Benefits**: Bengali Unicode preservation with OCR enhancement, page-specific extraction, systematic formatting cleanup through regex patterns and tesseract-ocr integration.

### ### Q2: What chunking strategy did you choose? Why do you think it works well for semantic retrieval?

**Answer**: I chose paragraph-based chunking after experimenting with multiple strategies. I tested 2-line based chunks, 1-line based chunks, and paragraph-based chunks, and found more accuracy in paragraph-based chunking for semantic retrieval.

#### **Chunking Experiments Conducted:**
```
data/chunks_2line.txt    # 2-line based chunking (testing)
data/chunks_small.txt    # 1-line based chunking (testing) 
data/chunks.txt          # Paragraph-based chunking (final)
```
#### **Chunking Strategy Comparison:**

| Strategy | File | Chunk Size | Semantic Coherence | Accuracy | Selected |
|----------|------|------------|-------------------|----------|----------|
| **1-line** | `chunks_small.txt` | ~20-50 words | ‚ùå Low | ‚ùå Poor | ‚ùå |
| **2-line** | `chunks_2line.txt` | ~40-100 words | ‚ö†Ô∏è Moderate | ‚ö†Ô∏è Good | ‚ùå |
| **Paragraph** | `chunks.txt` | ~100-200 words | ‚úÖ High | ‚úÖ **Best** | ‚úÖ |

#### **Why Paragraph-based Works Best:**

1. **Semantic Coherence**: Maintains complete thoughts and context
2. **Bengali Text Structure**: Related ideas in Bengali prose span multiple lines
3. **Embedding Quality**: Provides sufficient context for meaningful embeddings
4. **Query Matching**: Better alignment with user question complexity
5. **Answer Generation**: More complete context for LLM to generate accurate answers

#### **Final Chunking Code (Paragraph-based):**
```python
# File: app/chunk_text.py (Modified for paragraph-based)
input_path = 'data/cleaned_text.txt'
output_path = 'data/chunks.txt'

with open(input_path, 'r', encoding='utf-8') as f:
    text = f.read()

# Split by double newlines for paragraph-based chunking
paragraphs = text.split('\n\n')
chunks = []

for paragraph in paragraphs:
    paragraph = paragraph.strip()
    # Filter paragraphs with minimum length (more than 50 characters)
    if len(paragraph) > 50:
        chunks.append(paragraph)

# Save paragraph-based chunks
with open(output_path, 'w', encoding='utf-8') as f:
    for i, chunk in enumerate(chunks):
        f.write(f"---chunk_{i+1}---\n{chunk}\n")

print(f"Total {len(chunks)} paragraph-based chunks saved to {output_path}")
```

**Results**: Paragraph-based chunking achieved higher accuracy in retrieving relevant context for Bengali Q&A, especially for complex queries about character relationships and story details.

### Q3: What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?

**Answer**: I used `paraphrase-multilingual-MiniLM-L12-v2` because:
- Specifically designed for multilingual tasks including Bengali
- Optimized for semantic similarity tasks
- Lightweight yet effective (384-dimensional embeddings)
- Pre-trained on paraphrase data, ideal for Q&A scenarios
- Captures semantic meaning through transformer attention mechanisms

### Q4: How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?

**Answer**: I use:
- **FAISS IndexFlatL2** for efficient similarity search
- **Cosine similarity** for semantic comparison
- **Combined scoring** with keyword matching and semantic similarity
- FAISS provides fast approximate nearest neighbor search, essential for real-time responses

### Q5: How do you ensure that the question and document chunks are compared meaningfully? What would happen if the query is vague or missing context?

**Answer**: I ensure meaningful comparison through:
- Multi-metric scoring (semantic + keyword matching)
- Stop word removal to focus on content words
- Confidence scoring for answer reliability
- For vague queries: the system returns lower confidence scores and may provide generic answers or request clarification

### Q6: Do the results seem relevant? If not, what might improve them?

**Answer**: Results are quite relevant for the test cases. Potential improvements:
- Larger chunk overlap for better context
- Query expansion for Bengali synonyms
- Better preprocessing for Bengali-specific linguistics
- Fine-tuned embedding model on Bengali literature
- Hybrid search combining semantic and lexical matching

## ‚úÖ Task Completion Status

### Core Tasks
| Task | Status | Description |
|------|--------|-------------|
| ‚úÖ Multilingual Support | Completed | Supports both English and Bengali queries |
| ‚úÖ Document Retrieval | Completed | FAISS-based vector search with semantic similarity |
| ‚úÖ Answer Generation | Completed | GROQ LLM integration for contextual answers |
| ‚úÖ Knowledge Base | Completed | HSC26 Bangla 1st paper processed and vectorized |
| ‚úÖ Pre-processing | Completed | Text cleaning, chunking, and normalization |
| ‚úÖ Chunking & Vectorization | Completed | 2-line paragraph-based chunks with multilingual embeddings |
| ‚úÖ Memory Management | Completed | Vector database for long-term, session state for short-term |

### Bonus Tasks
| Task | Status | Description |
|------|--------|-------------|
| ‚úÖ REST API | Completed | Flask-based API with POST /api/search endpoint |
| ‚úÖ Web Interface | Completed | Modern responsive UI with real-time interaction |
| ‚úÖ RAG Evaluation | Completed | Multi-metric evaluation (confidence, similarity, keyword matching) |
| ‚úÖ Groundedness Check | Completed | Answer validation against retrieved context |
| ‚úÖ Relevance Scoring | Completed | Semantic similarity and keyword matching scores |

### Sample Test Results
| Query | Expected | System Response | Status |
|-------|----------|-----------------|--------|
| ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá? | ‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶• | ‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶• | Correct |
| ‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá? | ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá | ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá | Correct |
| ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤? | ‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞ | ‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞ | Incorrect |

## üîó API Documentation

### POST /api/search
```json
{
  "query": "‡¶¨‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶™‡¶≤‡¶ï‡ßç‡¶∑‡ßá ‡¶ï‡¶®‡ßç‡¶Ø‡¶æ‡¶™‡¶ï‡ßç‡¶∑‡¶ï‡ßá ‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º ‡¶Ü‡¶∏‡¶§‡ßá ‡¶π‡¶≤‡ßã?"
}
```

### Response Format
```json
{
  "answer": "‡¶ï‡¶≤‡¶ø‡¶ï‡¶æ‡¶§‡¶æ‡ßü",
  "confidence": 0.85,
  "keyword_matches": 2.0,
  "semantic_score": 0.751,
  "TF-IDF": 0.386,
  "matching_keywords": ["‡¶¨‡¶ø‡ßü‡ßá","‡¶â‡¶™‡¶≤‡¶ï‡ßç‡¶∑‡ßá", "‡¶ï‡¶®‡ßç‡¶Ø‡¶æ‡¶™‡¶ï‡ßç‡¶∑"],
  "full_chunk": "‡¶¨‡¶≤‡¶æ ‡¶¨‡¶æ‡¶π‡ßÅ‡¶≤‡ßç‡¶Ø,‡¶¨‡¶ø‡¶¨‡¶æ‡¶π-‡¶â‡¶™‡¶≤‡¶ï‡ßç‡¶∑‡ßá ‡¶ï‡¶®‡ßç‡¶Ø‡¶æ‡¶™‡¶ï‡ßç‡¶∑‡¶ï‡ßá‡¶á ‡¶ï‡¶≤‡¶ø‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶Ü‡¶∏‡¶ø‡¶§‡ßá ‡¶π‡¶á‡¶≤‡•§ ‡¶ï‡¶®‡ßç‡¶Ø‡¶æ‡¶∞ ‡¶™‡¶ø‡¶§‡¶æ ‡¶∂‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•‡¶¨‡¶æ‡¶¨‡ßÅ ‡¶π‡¶∞‡¶ø‡¶∂‡¶ï‡ßá ‡¶ï‡¶§ ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßá‡¶® ‡¶§‡¶æ‡¶π‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶Æ‡¶æ‡¶£ ‡¶è‡¶á ‡¶Ø‡ßá, ‡¶¨‡¶ø‡¶¨‡¶æ‡¶π‡ßá‡¶∞ ‡¶§‡¶ø‡¶® ‡¶¶‡¶ø‡¶® ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡ßá‡¶§‡¶ø‡¶®‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶ö‡¶ï‡ßç‡¶∑‡ßá ‡¶¶‡ßá‡¶ñ‡ßá‡¶® ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶∂‡ßÄ‡¶∞‡ßç‡¶¨‡¶æ‡¶¶ ‡¶ï‡¶∞‡¶ø‡ßü‡¶æ‡¶Ø‡¶æ‡¶®‡•§ ‡¶¨‡ßü‡¶∏ ‡¶§‡¶æ‡¶Å‡¶∞ ‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂‡ßá‡¶∞ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶è‡¶™‡¶æ‡¶∞‡ßá ‡¶¨‡¶æ ‡¶ì‡¶™‡¶æ‡¶∞‡ßá‡•§ ‡¶ö‡ßÅ‡¶≤ ‡¶ï‡¶æ‡¶Å‡¶ö‡¶æ, ‡¶ó‡ßã‡¶Å‡¶´‡ßá ‡¶™‡¶æ‡¶ï ‡¶ß‡¶∞‡¶ø‡¶§‡ßá ‡¶Ü‡¶∞‡¶Æ‡ßç‡¶≠ ‡¶ï‡¶∞‡¶ø‡ßü‡¶æ‡¶õ‡ßá‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡•§ ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶¨‡¶ü‡ßá‡•§ ‡¶≠‡¶ø‡ßú‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶¶‡ßá‡¶ñ‡¶ø‡¶≤‡ßá ‡¶∏‡¶ï‡¶≤‡ßá‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶§‡¶æ‡¶Å‡¶∞ ‡¶â‡¶™‡¶∞‡ßá ‡¶ö‡ßã‡¶ñ ‡¶™‡ßú‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶ö‡ßá‡¶π‡¶æ‡¶∞‡¶æ‡•§",
  "model": "llama-3.3-70b-versatile",
  "success": true
}
```

## üì¶ Technologies Used

- **Backend**: Flask, Python 3.8+
- **ML/AI**: sentence-transformers, FAISS, GROQ
- **Text Processing**: pdfplumber, NLTK, regex
- **Frontend**: HTML5, CSS3, JavaScript (Vanilla)
- **Database**: Pickle-based vector storage
- **API**: RESTful API with CORS support

## ü§ù Contributing

Feel free to submit issues, fork the repository, and create pull requests for any improvements.

## üìÑ License

This project is open source and available under the MIT License.